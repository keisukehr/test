{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: NWY4MTJmZGQwOGQ5Njk1NTI4YTY5MTFlOnlsc3FzOEJ0UzVLckNhbzU0blhPcHFZVkl3MEV4bFFpL1ZsNnpSTmdNclk9\n",
      "Endpoint: https://app.datarobot.com/api/v2\n",
      "Input: /Users/mon/Desktop/aia_test_02/input/lending_club_5000.csv\n",
      "Target: bad_loan\n",
      "metrics: AUC\n",
      "number of cv-folds: 5\n"
     ]
    }
   ],
   "source": [
    "### 1. 諸設定\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import settings.modeling_settings as ms\n",
    "\n",
    "# ランダムシードを何回振るか\n",
    "N_ITERATIONS = 1\n",
    "\n",
    "# 設定の表示\n",
    "print('API Key:', ms.DR_API_KEY)\n",
    "print('Endpoint:', ms.DR_END_POINT)\n",
    "print('Input:', ms.INPUT_FILE_PATH)\n",
    "print('Target:', ms.COL_TARGET)\n",
    "print('metrics:', ms.METRICS)\n",
    "print('number of cv-folds:', ms.N_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataRobot API バージョンは 2.22.1 を利用しています。\n",
      "エンドポイントは: https://app.datarobot.com/api/v2 TOKENは NWY4MTJmZGQwOGQ5Njk1NTI4YTY5MTFlOnlsc3FzOEJ0UzVLckNhbzU0blhPcHFZVkl3MEV4bFFpL1ZsNnpSTmdNclk9 を利用して接続しています。\n",
      "接続判定: True\n"
     ]
    }
   ],
   "source": [
    "### 2. 環境初期化\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datarobot as dr\n",
    "import datetime as dt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from pylab import rcParams\n",
    "from colour import Color\n",
    "import wordcloud\n",
    "pd.set_option('display.max_rows', 500)\n",
    "%matplotlib inline\n",
    "\n",
    "dr_dark_blue = '#08233F'\n",
    "dr_blue = '#1F77B4'\n",
    "dr_orange = '#FF7F0E'\n",
    "dr_red = '#BE3C28'\n",
    "\n",
    "def __init_datarobot_env(token):\n",
    "    c = dr.Client(endpoint=ms.DR_END_POINT, token=ms.DR_API_KEY)\n",
    "    print(\"DataRobot API バージョンは\", dr.__version__, \"を利用しています。\")\n",
    "    print(\"エンドポイントは:\", c.endpoint, \"TOKENは\", c.token, \"を利用して接続しています。\")\n",
    "    print(\"接続判定:\", c.verify)\n",
    "    \n",
    "__init_datarobot_env(ms.DR_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. データ読み込み\n",
    "\n",
    "def __read_inputs():\n",
    "    def read_file_autohandle(input_path):\n",
    "        _, ext = os.path.splitext(input_path)\n",
    "        if ext == \".csv\":\n",
    "            result = pd.read_csv(\n",
    "                input_path,encoding=\"utf-8-sig\")\n",
    "            return result\n",
    "        elif (ext == \".xls\") or (ext == \".xlsx\"):\n",
    "            result = pd.read_excel(input_path, encoding=\"utf-8-sig\")\n",
    "            return result\n",
    "\n",
    "    df_x = read_file_autohandle(ms.INPUT_FILE_PATH)\n",
    "    df_y = df_x[ms.COL_TARGET]\n",
    "    df_id = []\n",
    "    for col in df_x.columns:\n",
    "        df_x.rename(columns={col:col.replace(\"{\",\"_\").replace(\"-\",\"_\").replace(\"$\",\"_\").replace(\".\",\"_\").replace(\"}\",\"_\").replace(\"\\n\",\"_\").replace('\"',\"_\")},inplace=True)\n",
    "    return (df_id, df_x, df_y)\n",
    "\n",
    "df_id,df_x,df_y=__read_inputs()\n",
    "dict_log_input = {\n",
    "    'input_file_path': ms.INPUT_FILE_PATH,\n",
    "    'n_records': len(df_x),\n",
    "    'rate_target': sum(df_x[ms.COL_TARGET])/len(df_x)\n",
    "}\n",
    "pd.to_pickle(dict_log_input, \"../output/intermediate_files/input_info.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. オートパイロット・交差検定実行\n",
    "\n",
    "def __run_autopilot():\n",
    "    print(\"Start creating projects...\")\n",
    "    project_id_list = []\n",
    "    for i in range(1, N_ITERATIONS+1):\n",
    "        project_name = str(dt.date.today()).replace('-', '') + '_' + ms.INPUT_FILE_PATH[ms.INPUT_FILE_PATH.rfind('/')+1:] + '_Seed' + str(i)\n",
    "        project = dr.Project.create(df_x, project_name=project_name)\n",
    "        project.set_worker_count(-1)\n",
    "        ao = dr.AdvancedOptions(seed=i)\n",
    "        pm = dr.StratifiedCV(holdout_pct=0, reps=ms.N_FOLDS, seed=i)\n",
    "        project.set_target(ms.COL_TARGET, metric=ms.METRICS, partitioning_method=pm, advanced_options=ao, mode=dr.AUTOPILOT_MODE.FULL_AUTO)\n",
    "        print(\"The new project has ID for seed \"+str(i)+\" is:\", project.id)\n",
    "        project_id_list.append(project.id)\n",
    "        random_seeds.append(i)\n",
    "        project_ids.append(project.id)\n",
    "        project_names.append(project_name)\n",
    "    print(\"project IDs are \" + str(project_id_list))\n",
    "    return project_id_list\n",
    "\n",
    "def __get_model_scores(project):\n",
    "    return pd.DataFrame(\n",
    "        [[model.metrics[project.metric]['crossValidation'],\n",
    "          model.metrics[project.metric]['validation'],\n",
    "          model.model_type,\n",
    "          model.id,\n",
    "          model.sample_pct,\n",
    "          model.model_category,\n",
    "          model,\n",
    "          model.blueprint_id\n",
    "          ] for model in project.get_models(with_metric=project.metric)],\n",
    "        columns=['cv', 'v', 'type', 'model_id', 'sample_pct',\n",
    "                 'category', 'model', 'blueprint_id']\n",
    "    ).sort_values(['cv', 'v'], na_position='last')\n",
    "\n",
    "def __run_cross_validation(project_id_list):\n",
    "    for i in range(0, N_ITERATIONS):\n",
    "        project = dr.Project.get(project_id=project_id_list[i])\n",
    "        project.wait_for_autopilot(check_interval=60.0)\n",
    "        print(\"Autopilot of the project for seed \"+str(i+1)+\" is completed\")\n",
    "        print(\"Confirming CV status....\")\n",
    "        jobs_list = project.get_all_jobs()\n",
    "        for job in jobs_list:\n",
    "            job.wait_for_completion(max_wait=60000)\n",
    "        df_model = __get_model_scores(project)\n",
    "        if max(df_model[\"sample_pct\"])==100:\n",
    "            df_model = df_model[df_model[\"sample_pct\"] == sorted(list(set(df_model[\"sample_pct\"])))[-2]]\n",
    "            df_model = df_model[df_model[\"category\"] == \"model\"]\n",
    "            for model in (df_model[df_model[\"cv\"].isnull()][\"model\"]):\n",
    "                print(\"Seed\"+str(i+1)+\" \"+model.model_type+\" started CV\")\n",
    "                model.cross_validate()\n",
    "        else:\n",
    "            print(\"Seed \"+str(i+1)+\" looks error occured. Ignored\")\n",
    "            project_id_list[i]=\"error\"\n",
    "            continue\n",
    "            \n",
    "def __wait_for_cv(project_id_list):\n",
    "    for i in range(0, N_ITERATIONS):\n",
    "        project = dr.Project.get(project_id=project_id_list[i])\n",
    "        jobs_list = project.get_all_jobs()\n",
    "        for job in jobs_list:\n",
    "            job.wait_for_completion(max_wait=60000)\n",
    "        print(\"completed CV of in seed\"+str(i+1))\n",
    "\n",
    "#random_seeds = []\n",
    "#project_ids = []\n",
    "#project_names = []\n",
    "\n",
    "#project_id_list=__run_autopilot()\n",
    "#__run_cross_validation(project_id_list)\n",
    "#__wait_for_cv(project_id_list)\n",
    "#dict_log_project = {\n",
    "#    'random_seed': random_seeds,\n",
    "#    'project_id': project_ids,\n",
    "#    'project_name': project_names\n",
    "#}\n",
    "#pd.to_pickle(dict_log_project, '../output/intermediate_files/project_info.pkl')\n",
    "\n",
    "project_id_list=['5f85e73425132a0427978946']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed seed1\n",
      "Request and get feature impact for seed1\n"
     ]
    }
   ],
   "source": [
    "### 5. 特徴量のインパクトを算出\n",
    "\n",
    "N_BEST_MODELS = 3 # 上位いくつのモデルを参照して重要な特徴量を決めるか\n",
    "N_TOP_FEATURES = 10 # 複数モデル共通で重要度topNの特徴量は、重要とみなす。その時のN\n",
    "\n",
    "def __check_impact(project_id_list, save_histgrams=True):\n",
    "    i=0\n",
    "    feature_impacts = [{} for j in range(N_ITERATIONS)]\n",
    "    project = dr.Project.get(project_id=project_id_list[i])\n",
    "    jobs_list = project.get_all_jobs()\n",
    "    for job in jobs_list:\n",
    "        job.wait_for_completion(max_wait=60000)\n",
    "    print(\"Completed seed\"+str(i+1))\n",
    "    df_model = __get_model_scores(project)\n",
    "    df_model['type'] = df_model['type'].str.replace(' / ', ', ')\n",
    "    target_pct = sorted(df_model['sample_pct'].unique())[::-1][1]\n",
    "    df_model = df_model[df_model['sample_pct'] == target_pct]\n",
    "    if ms.METRICS == 'AUC':\n",
    "        df_model = df_model.sort_values('cv', ascending=False)\n",
    "    df_model = df_model[:N_BEST_MODELS]\n",
    "    print(\"Request and get feature impact for seed\"+str(i+1))\n",
    "    for m, model_id in enumerate(df_model['model_id']):\n",
    "        model = dr.Model.get(project_id_list[i], model_id)\n",
    "        feature_impact = model.get_or_request_feature_impact(max_wait=60000, row_count=100000)\n",
    "        feature_impacts[i][model_id] = feature_impact\n",
    "        if save_histgrams:\n",
    "            percent_tick_fmt = mtick.PercentFormatter(xmax=1.0)\n",
    "            impact_df = pd.DataFrame(feature_impact)\n",
    "            impact_df.sort_values(by='impactNormalized', ascending=True, inplace=True)\n",
    "            bar_colors = impact_df.impactNormalized.apply(lambda x: dr_red if x < 0\n",
    "                                                          else dr_blue)\n",
    "            ax = impact_df.plot.barh(x='featureName', y='impactNormalized',\n",
    "                                     legend=False,\n",
    "                                     color=bar_colors,\n",
    "                                     figsize=(16, 8))\n",
    "            ax.xaxis.set_major_formatter(percent_tick_fmt)\n",
    "            ax.xaxis.set_tick_params(labeltop=True)\n",
    "            ax.xaxis.grid(True, alpha=0.2)\n",
    "            ax.set_facecolor(dr_dark_blue)\n",
    "            plt.ylabel('')\n",
    "            plt.xlabel('Effect')\n",
    "            plt.xlim((None, 1))  # Allow for negative impact\n",
    "            plt.title('Feature Impact', y=1.04)\n",
    "            plt.savefig('../output/intermediate_files/feature_impacts_rank_{rank}_{model_name}.png'.format(rank=m+1, model_name=df_model['type'].iloc[m].replace(' ', '_')))\n",
    "            plt.close()\n",
    "    return feature_impacts\n",
    "\n",
    "def __get_common_top_features(feature_impacts, n_top=10):\n",
    "    commonly_important_features = [[] for i in range(0, N_ITERATIONS)]\n",
    "    for i in range(0, N_ITERATIONS):\n",
    "        best_models_important_features = None\n",
    "        for m, (model_id, important_features) in enumerate(feature_impacts[i].items()):\n",
    "            important_features = [feature['featureName'] for feature in important_features][:n_top]\n",
    "            commonly_important_features[i] += important_features\n",
    "            if m == 0:\n",
    "                best_models_important_features = important_features\n",
    "        commonly_important_features[i] = list(set(commonly_important_features[i])) # 重複削除\n",
    "        commonly_important_features[i] = [f for f in best_models_important_features if f in commonly_important_features[i]] # ソート\n",
    "    return commonly_important_features\n",
    "            \n",
    "feature_impacts=__check_impact(project_id_list)\n",
    "commonly_important_features=__get_common_top_features(feature_impacts, n_top=N_TOP_FEATURES)\n",
    "pd.to_pickle(commonly_important_features, \"../output/intermediate_files/commonly_important_features.pkl\") # 共通で重要な特徴量の一覧を保存しておく。読み出し：arr_tmp = pd.read_pickle(\"commonly_important_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting feature effect seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-c39cd5fda900>:58: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  df=pd.io.json.json_normalize(feature_effects[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000.0 3500000.0\n",
      "2500000.0 15488000.000000013\n",
      "234585.0 4198123.000000003\n",
      "4.56 31.73\n",
      "4010300.12 4989761.1499999985\n",
      "0.0 6.0\n",
      "4.0 24.0\n"
     ]
    }
   ],
   "source": [
    "### 6. 最も精度の良いモデルの特徴量ごとの作用を算出\n",
    "\n",
    "def __get_file_name(feature_name, graph_type):\n",
    "    if graph_type == 'partial_dependence':\n",
    "        return '../output/intermediate_files/partial_dependence_{}.png'.format(feature_name)\n",
    "    elif graph_type == 'histgram':\n",
    "        return '../output/intermediate_files/partial_dependence_hist_{}.png'.format(feature_name)\n",
    "    else:\n",
    "        return \n",
    "    \n",
    "def __get_feature_effects(project_id_list):\n",
    "    best_model_feature_effects = [None]*N_ITERATIONS\n",
    "    # request\n",
    "    for i in range(0, N_ITERATIONS):\n",
    "        if project_id_list[i]==\"error\":\n",
    "            best_model_feature_effects[i]=\"error\"\n",
    "            continue\n",
    "        project = dr.Project.get(project_id=project_id_list[i])\n",
    "        df_model = __get_model_scores(project)\n",
    "        df_model['type'] = df_model['type'].str.replace(' / ', ', ')\n",
    "        target_pct = sorted(df_model['sample_pct'].unique())[::-1][1]\n",
    "        df_model = df_model[df_model['sample_pct'] == target_pct]\n",
    "        if ms.METRICS == 'AUC':\n",
    "            df_model = df_model.sort_values('cv', ascending=False)\n",
    "        model = dr.Model.get(project_id_list[i], df_model.iloc[0, 3])\n",
    "        model.get_feature_effect_metadata()\n",
    "        model.request_feature_effect()\n",
    "    # get results\n",
    "    for i in range(0, N_ITERATIONS):\n",
    "        if project_id_list[i]==\"error\":\n",
    "            best_model_feature_effects[i]=\"error\"\n",
    "            continue\n",
    "        project = dr.Project.get(project_id=project_id_list[i])\n",
    "        jobs_list = project.get_all_jobs()\n",
    "        print(\"Waiting feature effect seed \"+str(i+1))\n",
    "        for job in jobs_list:\n",
    "            job.wait_for_completion(max_wait=60000)\n",
    "        df_model = __get_model_scores(project)\n",
    "        df_model['type'] = df_model['type'].str.replace(' / ', ', ')\n",
    "        target_pct = sorted(df_model['sample_pct'].unique())[::-1][1]\n",
    "        df_model = df_model[df_model['sample_pct'] == target_pct]\n",
    "        if ms.METRICS == 'AUC':\n",
    "            df_model = df_model.sort_values('cv', ascending=False)\n",
    "        model = dr.Model.get(project_id_list[i], df_model.iloc[0, 3])\n",
    "        best_model_feature_effects[i] = model.get_feature_effect(\"validation\")\n",
    "\n",
    "    return best_model_feature_effects\n",
    "\n",
    "def __create_effect_plot(important_features, feature_effects):\n",
    "    max_dependence=0\n",
    "    min_dependence=100\n",
    "    \n",
    "    for x in important_features:\n",
    "        effect=pd.DataFrame()\n",
    "        for i in range(0,N_ITERATIONS):\n",
    "            if project_id_list[i]==\"error\":\n",
    "                continue\n",
    "            df=pd.io.json.json_normalize(feature_effects[i])\n",
    "            if x in list(df[\"feature_name\"]):\n",
    "                z=pd.DataFrame()\n",
    "                z=list(df[df[\"feature_name\"]==x][\"partial_dependence.data\"])\n",
    "\n",
    "                def flatten_2d(data):\n",
    "                    for block in data:\n",
    "                        for elem in block:\n",
    "                            yield elem\n",
    "\n",
    "                z = list(flatten_2d(z))\n",
    "                z=  pd.DataFrame(z)\n",
    "                effect[[\"label\",\"seed\"+str(i+1)]]=z[[\"label\",\"dependence\"]]\n",
    "                if max_dependence<z[\"dependence\"].max():\n",
    "                    max_dependence=z[\"dependence\"].max() \n",
    "                if min_dependence>z[\"dependence\"].min():\n",
    "                    min_dependence=z[\"dependence\"].min() \n",
    "                if list(df[df[\"feature_name\"]==x][\"feature_type\"])==[\"numeric\"]:\n",
    "                    effect[\"label\"]=effect[\"label\"].astype(\"float\")\n",
    "                else:\n",
    "                    def order(effect,df_x,x):\n",
    "                        l_order = list(df_x[x].value_counts().index)\n",
    "                        j=0\n",
    "                        l_order_dic={}\n",
    "                        for x in l_order:\n",
    "                            l_order_dic[x]=j\n",
    "                            j+=1\n",
    "                        effect['order'] = effect['label'].map(l_order_dic)\n",
    "                        effect[\"order\"]=effect[\"order\"].fillna(j)\n",
    "                        effect=effect.sort_values(\"order\")\n",
    "                        effect.drop(columns=\"order\",inplace=True)\n",
    "                        return effect\n",
    "                    \n",
    "                    effect=order(effect,df_x,x)\n",
    "                    \n",
    "        if x in list(df[\"feature_name\"]):\n",
    "            effect.to_csv('../output/intermediate_files/partial_dependence_{}.csv'.format(x),index=False,encoding='utf-8-sig')\n",
    "            \n",
    "    for x in important_features:\n",
    "        if x in list(df[\"feature_name\"]):\n",
    "            effect=pd.read_csv('../output/intermediate_files/partial_dependence_{}.csv'.format(x),encoding='utf-8-sig')\n",
    "            \n",
    "            if effect[\"label\"].nunique()!=1:\n",
    "                plt.rcParams['font.family'] = 'IPAexGothic'\n",
    "                plt.rcParams['font.size'] = 15\n",
    "                plt.rcParams['lines.linewidth'] = 1\n",
    "                plt.rcParams['figure.figsize'] = 8,6\n",
    "                plt.rcParams['axes.facecolor'] = dr_dark_blue\n",
    "                if list(df[df[\"feature_name\"]==x][\"feature_type\"])==[\"numeric\"]:\n",
    "                    for y in effect.columns[1:]:\n",
    "                        if y == effect.columns[-1]:\n",
    "                            plt.rcParams['lines.linewidth'] = 5\n",
    "                        plt.plot(effect[\"label\"], effect[y], label = y, color=dr_orange)\n",
    "                else:\n",
    "                    for y in effect.columns[1:]:\n",
    "                        if y == effect.columns[-1]:\n",
    "                            plt.scatter(effect[\"label\"], effect[y], label = y, s=100, color=dr_orange)\n",
    "                        else:\n",
    "                            plt.scatter(effect[\"label\"], effect[y], label = y, color=dr_orange)\n",
    "                plt.xticks(rotation=90, size='small')\n",
    "                plt.title(x+\"の作用\")\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                plt.ylim([min_dependence,max_dependence])\n",
    "                plt.savefig(__get_file_name(x, graph_type='partial_dependence'),bbox_inches=\"tight\")\n",
    "                #plt.show()\n",
    "                plt.close()\n",
    "                \n",
    "                plt.rcParams['axes.facecolor'] = dr_dark_blue\n",
    "                if list(df[df[\"feature_name\"]==x][\"feature_type\"])==[\"numeric\"]:\n",
    "                    df_x[x].hist(bins=10)\n",
    "                    print(effect[\"label\"].min(),effect[\"label\"].max())\n",
    "                else:\n",
    "                    df_x[x].value_counts().plot(kind=\"bar\", color=dr_blue)\n",
    "                plt.title(x+\"のヒストグラム\")\n",
    "                plt.savefig(__get_file_name(x, graph_type='histgram'),bbox_inches=\"tight\")\n",
    "                #plt.show()\n",
    "                plt.close()\n",
    "            \n",
    "    return\n",
    "\n",
    "def __create_wordcloud(project_id_list):\n",
    "    i=0\n",
    "    \n",
    "    def word_cloud_plot(wc, font_path=None):\n",
    "        # Stopwords usually dominate any word cloud, so we will filter them out\n",
    "        dict_freq = {wc_word['ngram']: wc_word['frequency']\n",
    "                     for wc_word in wc.ngrams\n",
    "                     if not wc_word['is_stopword']}\n",
    "        dict_coef = {wc_word['ngram']: wc_word['coefficient']\n",
    "                     for wc_word in wc.ngrams}\n",
    "\n",
    "        def color_func(*args, **kwargs):\n",
    "            word = args[0]\n",
    "            palette_index = int(round(dict_coef[word] * 100)) + 100\n",
    "            r, g, b = colors[palette_index].get_rgb()\n",
    "            return 'rgb({:.0f}, {:.0f}, {:.0f})'.format(int(r * 255),\n",
    "                                                        int(g * 255),\n",
    "                                                        int(b * 255))\n",
    "\n",
    "        wc_image = wordcloud.WordCloud(stopwords=set(),\n",
    "                                       width=1024, height=1024,\n",
    "                                       relative_scaling=0.5,\n",
    "                                       prefer_horizontal=1,\n",
    "                                       color_func=color_func,\n",
    "                                       background_color=(0, 10, 29),\n",
    "                                       font_path=font_path).fit_words(dict_freq)\n",
    "        plt.imshow(wc_image, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.savefig('../output/intermediate_files/word_cloud.png',bbox_inches='tight')\n",
    "        plt.close()\n",
    "        return \n",
    "        \n",
    "    colors = [Color('#2458EB')]\n",
    "    colors.extend(list(Color('#2458EB').range_to(Color('#31E7FE'), 81))[1:])\n",
    "    colors.extend(list(Color('#31E7FE').range_to(Color('#8da0a2'), 21))[1:])\n",
    "    colors.extend(list(Color('#a18f8c').range_to(Color('#ffad9e'), 21))[1:])\n",
    "    colors.extend(list(Color('#ffad9e').range_to(Color('#d80909'), 81))[1:])\n",
    "    webcolors = [c.get_web() for c in colors]\n",
    "        \n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    dr_cmap = LinearSegmentedColormap.from_list('DataRobot', webcolors, N=len(colors))\n",
    "        \n",
    "    project = dr.Project.get(project_id=project_id_list[i])\n",
    "    models = project.get_models()\n",
    "    model_with_word_cloud = None\n",
    "    for model in models:\n",
    "        if 'Auto-Tuned Word N-Gram Text Modeler' in model.model_type:\n",
    "            try:\n",
    "                model.get_word_cloud()\n",
    "                model_with_word_cloud = model\n",
    "                break\n",
    "            except ClientError as e:\n",
    "                if e.json['message'] and 'No word cloud data' in e.json['message']:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise\n",
    "    wc = model_with_word_cloud.get_word_cloud(exclude_stop_words=True)\n",
    "    #word_cloud_plot(wc, font_path='C:/Windows/Fonts/ipaexg.ttf')\n",
    "    word_cloud_plot(wc, font_path='/Users/mon/Library/Fonts/ipaexg.ttf')\n",
    "    return \n",
    "\n",
    "feature_effects=__get_feature_effects(project_id_list)\n",
    "__create_effect_plot(commonly_important_features[0], feature_effects)\n",
    "__create_wordcloud(project_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Logファイルを作成・保存\n",
    "\n",
    "def __create_log(project_id_list):    \n",
    "    # 必要な情報の読み込み\n",
    "    prj_ids, iterations, model_ranks, model_names, model_ids, sample_pcts = [], [], [], [], [], []\n",
    "    metric_names, cv_scores, f1scores, precisions, recalls = [], [], [], [], []\n",
    "    for i in range(0, N_ITERATIONS):\n",
    "        if project_id_list[i]==\"error\":\n",
    "            prj_ids.append(\"error\")\n",
    "            iterations.append(\"error\")\n",
    "            model_ranks.append(\"error\")\n",
    "            model_names.append(\"error\")\n",
    "            metric_names.append(\"error\")\n",
    "            cv_scores.append(\"error\")\n",
    "            model_ids.append(\"error\")\n",
    "            sample_pcts.append(\"error\")\n",
    "            f1scores.append(\"error\")\n",
    "            precisions.append(\"error\")\n",
    "            recalls.append(\"error\")\n",
    "            continue\n",
    "        project = dr.Project.get(project_id=project_id_list[i])\n",
    "        df_model = __get_model_scores(project)\n",
    "        df_model['type'] = df_model['type'].str.replace(' / ', ', ')\n",
    "        target_pct = sorted(df_model['sample_pct'].unique())[::-1][1]\n",
    "        df_model = df_model[df_model['sample_pct'] == target_pct]\n",
    "        if ms.METRICS == 'AUC':\n",
    "            df_model = df_model.sort_values('cv', ascending=False)\n",
    "        for m in range(N_BEST_MODELS):\n",
    "            model = dr.Model.get(project=project_id_list[i], model_id=df_model['model_id'].iloc[m])\n",
    "            roc = model.get_roc_curve('crossValidation')\n",
    "            threshold = roc.get_best_f1_threshold()\n",
    "            metrics = roc.estimate_threshold(threshold)\n",
    "            \n",
    "            prj_ids.append(project_id_list[i])\n",
    "            iterations.append(i)\n",
    "            model_ranks.append(m+1)\n",
    "            model_names.append(df_model['type'].iloc[m])\n",
    "            model_ids.append(df_model['model_id'].iloc[m])\n",
    "            sample_pcts.append(df_model['sample_pct'].iloc[m])\n",
    "            metric_names.append(project.metric)\n",
    "            cv_scores.append(model.metrics[project.metric][\"crossValidation\"])\n",
    "            f1scores.append(metrics['f1_score'])\n",
    "            precisions.append(metrics['true_positive_score']/(metrics['true_positive_score']+metrics['false_positive_score']))\n",
    "            recalls.append(metrics['true_positive_score']/(metrics['true_positive_score']+metrics['false_negative_score']))\n",
    "    # DataFrameの作成\n",
    "    df_log_model = pd.DataFrame({\n",
    "        'project_id': prj_ids,\n",
    "        'random_seed': iterations,\n",
    "        'model_id': model_ids,\n",
    "        'model_rank': model_ranks,\n",
    "        'model_name': model_names,\n",
    "        'sample_pct': sample_pcts,\n",
    "        'metric': metric_names,\n",
    "        'cv_score': cv_scores,\n",
    "        'f1_score': f1scores,\n",
    "        'precision': precisions,\n",
    "        'recall': recalls\n",
    "    })\n",
    "    df_log_model['image_path_impact'] = '../output/intermediate_files/feature_impacts_rank_' + df_log_model['model_rank'].astype(str) + '_' + df_log_model['model_name'].str.replace(' ', '_') + '.png'\n",
    "    return df_log_model\n",
    "        \n",
    "df_log_model = __create_log(project_id_list) \n",
    "df_log_model.to_csv('../output/intermediate_files/model_info.csv',index=False,encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
